lines(c(0,500),c(5,5),col=2,lwd=2)
lines(N,lb.est,col=3,lty=2,lwd=2)
lines(N,ub.est,col=3,lty=2,lwd=2)
#####HYPOTHESIS TESTS
#####################
##F test
#Important assumption that the data is normally distributed
#this is all saying it is greater than
type_A <- c(50, 75, 100, 35, 90, 20)
type_B <- c(70, 77, 80, 65, 71)
#calculating the variance of type A
var_1 <- var(type_A)
var_1
#Calculating the variance of type B
var_2 <- var(type_B)
var_2
#Calculating n
n_1 <- length(type_A)
n_2 <- length(type_B)
#F test statistic is the variance of 1 over the other
f.test <- var_1 / var_2
f.test
#degrees of freedom
df_1 <- n_1 - 1
df_2 <- n_2 - 1
#calculating the critical value for alpha = 0.05 with degrees of freedom df_1, df_2
qf(0.95, df_1, df_2)
#28.5 (f statistic) is greater than 6.25 (critical) so we can reject the null hypothesis
1-pf(f.test, df_1, df_2)
#thsi calculates the P-value which is less than 0.05 so we can reject
###ALTERNATIVE FISHER TEST pg 8
var.test(type_A, type_B, alternative = "greater")
#Gives you the F statistic, the p-value
#We can also see the ratio of varaince is between the confidence intervals
#Confirms we can reject the null hypothesis
##Exercise 3 pg 8
#one tailed test F-test for gardens dataset
#read in gardens data set
gardens <- read.csv2("gardens.csv", sep = ",", stringsAsFactors = T)
head(gardens)
#Run one-side F -test between garden B and C
str(gardens) #all ints so can do
#saying garden B is significantly greater than garden C
var.test(gardens$gardenB, gardens$gardenC, alternative = "less")
#p-value is 0.0008 which means we can reject the null
#two-sided
var.test(gardens$gardenB, gardens$gardenC)
#P-value is 0.0016 < 0.05
#so we can reject the null
##Exercise 4
#I thinkl the two-tailed test is the better one to use as we don't which greater
#less than the other before we start
####HYPOTHESIS TEST FOR A SINGLE MEAN
## T-test for single mean
type_A <- c(50, 75, 100, 35, 90, 20)
mean_A <- mean(type_A)
mean_A
#calculate sd
sd_A <- sd(type_A)
#calculate sample size
n_A <- length(type_A)
#Calculate standard error
se_A <- sd_A / sqrt(n_A)
mu_A <- 70 #mean assumption for null hypothesis
#calculate t statistic
t.stat <- (mean_A - mu_A)/ (se_A)
t.stat
#degrees of freedom (n-1)
df_A <- n_A - 1
df_A
#Calculate P-value
2*pt(t.stat, df_A)
#P-value = 0.548 whihc is larger than 0.05. So we have no evidence to reject null
##Alternative Way
#states one dataset and predicts a mean
t.test(type_A, mu = 70)
#p-value once again 0.548
#Once again can't reject the true mean
#the mean value is also between the confidence intervals
##Exercise 5 -practicing hypothesis test for a single mean
type_C = c(70, 77, 80, 65, 71)
t.test(type_A, mu = 68)
#p-value = 0.65 so we cannot reject the null
#b - if the mean is 50 or higher, so one-sided
type_d = c(60, 65, 65, 70, 100)
t.test(type_A, mu = 50, alternative = "greater")
#p-value is 0.2045 so we cannot reject the null that u = 50
#c - if type d mean 90 or lower
t.test(type_A, mu = 90, alternative = "less")
#p-value = 0.04 so we can reject the null at 95% confidence interval
####Hypothesis test for comparing the means of two samples
type_A <- c(50, 75, 100, 35, 90, 20)
type_B <- c(70, 77, 80, 65, 71)
#made the assumption that the two variances are not equal
mean_A <- mean(type_A)
mean_A
mean_B <- mean(type_B)
mean_B
#Sample size
n_A <- length(type_A)
n_B <- length(type_B)
#Variances
var_A <- var(type_A)
var_A
var_B <- var(type_B)
var_B
numerator_t <- mean_A - mean_B
denominator_t <- sqrt(var_A / n_A + var_B / n_B)
t_test <- numerator_t / denominator_t
t_test
#calculating the degrees of freedom
df_num<- (var_A/n_A+var_B/n_B)^2
df_denom<- (var_A/n_A)^2/(n_A-1)+(var_B/n_B)^2/(n_B-1)
df_<- df_num/df_denom
df_
#calculating the critical for alpha = 0.05 with degrees if freedon df_1, df_2
qt(0.95, df_)
#critical value = 1.98
#calculating the p-value
2*pt(t_test, df_)
# =0.44 which is large than 0.05 so we cannot reject the null
##Alternative way
t.test(type_A, type_B)
#once again shows we cannot reject the null
##Exercise 6 - two sample unpaired t-test
type_c <- c(70, 75, 80, 75, 76, 71)
type_d <- c(70, 77, 69, 68, 71)
#Are the two variances equal?
var.test(type_c, type_d)
#we cannot reject the null hypothesis that the variances are equal
mean_c <- mean(type_c)
mean_d <- mean(type_d)
mean_c
mean_d
#Are the two population means different
#Null - they are the same
t.test(type_c, type_d)
#cannot reject the null that means are equal
#b
#mistake in type d
type_d <- c(60, 77, 61, 68, 100)
#retest
var.test(type_c, type_d)
#variances are not equal and therefore we do a different test
#use Welch's t-test by var.equal = FALSE
t.test(type_c, type_d, var.equal = FALSE)
#P-value is not less than 0.05 so we cannot reject the null
#c hypothesis - indidviduals with coronary heart disease have a higher BMI than
#those without coronary heart disease
CHD <- read.csv2("CHD.csv", sep = ",", stringsAsFactors = T)
head(CHD)
str(CHD)
#Chagning obesity (BMI to an integer)
CHD$obesity <- as.numeric(as.character(CHD$obesity))
#getting the subet of BMI where chd = 1 i.e., has cardiovascular disease
obesity_chd <- CHD$obesity[CHD$chd == 1]
obesity_nochd <- CHD$obesity[CHD$chd == 0]
#checking this has worked
head(obesity_chd)
head(obesity_nochd)
t.test(obesity_chd, obesity_nochd, "greater")
#p-value = 0.02 so we cna reject the null that the means are the same
#### Non-parametric tests
#Mann-Whitney U test
#########################
#For large sample values (both samples greater than 20),
#you can have a normal approximation
#example using type A and B from the apple trees earlier
wilcox.test(type_A, type_B)
#p-value = 0.7922 so we cannot reject the null hypothesis
##Exercise 7
chocolate_A <- c(5, 4, 8, 9, 10, 3)
chocolate_B <- c(1, 4, 3, 5, 2, 1)
#because values are tied for this test you need include correct = TRUE and
#exact = FALSE
#correct = TRUE stated we have tied values
#exact = false states we can't provide an exact result
wilcox.test(chocolate_A, chocolate_B, exact = FALSE, correct = TRUE)
#p-value is less than 0.05 so we can reject the null
#b
male_smokers <- c(10, 15, 20, 40, 2)
female_smokers <- c(5, 10, 20, 25, 20)
#tied values again
wilcox.test(male_smokers, female_smokers, exact = FALSE, correct = TRUE)
#p-value =0.9155 so we cannot reject the null
#c
goldfish_1 <- c(100, 50, 70, 40)
goldfish_2 <- c(30, 50, 65, 125)
wilcox.test(goldfish_1, goldfish_2, exact = FALSE, correct = TRUE)
#p-value = 1 so we cannot reject the null hypothesis
##### Paired t-test - dependent means
#####################################
group1 <- c(140, 150, 145, 150, 160)
group2 <- c(140, 145, 140, 150, 160)
#testing with the thought group 1 greater than group 2
#paited = T is for paired T test
t.test(group1, group2, paired = T, alternative = "greater")
#p-value = 0.0889 so we cannot reject the null
#use a paired t-test when the two samples are not independent
#and we want both datasets are normally distributed
##Exercise 8
#a pg 15
bef_revision <- c(70, 75, 80, 75, 76, 71)
aft_revision <- c(70, 77, 69, 68, 71, 71)
#checking variance is equal, but would assume as it is same class
var.test(bef_revision, aft_revision)
#p-values = 0.7742 so we cannot reject the null that var is equal
#do paired t-test as dependent
t.test(bef_revision, aft_revision, paired = T)
#p-value = 0.1477 so we can not reject null that means are same
#b
aft_revision <- c(60, 77, 61, 68, 100, 100)
#assuming the var is not equal
t.test(bef_revision, aft_revision, paired = T, var.equal = F)
#p-value = 0.705 so we cannot reject the null
####Non-parametric test = The wilcoxon matcched pairs test
#this is applied ot non-parametric dependent samples
group_1 <- c(140, 150, 145, 151, 160)
group_2 <- c(139, 138, 141, 142, 159)
#have to add exact = F for this one as well
wilcox.test(group1, group_2, paired = T, exact = F)
#p-value = 0.06 so we cannot reject null, but maybe do another experiment with
#bigger sample size
#wilcoxon rank-sum test is for two independent samples whereas wilcoxon signed
#rank is for two dependent samples
##Exercise 9
bef_revision <- c(70, 75, 80, 75, 76, 71)
aft_revision <- c(70, 77, 69, 68, 71, 71)
#compare with with paired t-test as think they are normally distributed
t.test(bef_revision, aft_revision, paired = T)
#p-value = 0.1477 so we cannot reject the null
aft_revision_c <- c(60, 77, 61, 68, 100, 100)
wilcox.test(bef_revision, aft_revision_c, paired = T, exact = F)
#p-value = 0.8339 so we cannot reject the null hypothesis
#variance doe snot matter for these ones as they are ranked
####HYPOTHESIS TEST FOR A SINGLE PROPORTION
#pg 17
binom.test(90, 200, alternative = "two.sided", conf.level = 0.95)
#p-value = 0.179 so cannot reject null
#also probability pof success lies within the confidence intervals
#in detail, computing the z statistic
k <- 90 # 90 women-succes
n <- 200 # smaple size
pbar <- k/n #smaple proportion
p0 <- 0.5 #hypothesised value
z <- (pbar - p0)/ sqrt(p0 * (1 - p0) / n)
z
#can compute the critical value
alpha <- 0.05
alpha<- 0.05
z.half.alpha<- qnorm(1-alpha/2)
c(-z.half.alpha,z.half.alpha) #standard normal is symmetric around 0
#the z statistic lies between -1.96 and 1.96 so we cannot reject the null
#fo this z statistic need the approximation that np and n(1-p) are both
#greater than 5
#Z-test can do in alternative way
prop.test(90, 200, p = 0.5, correct = T)
#p-value is 0.1791 so we cannot reject the null
#the sample estimate is also in the confidence intervals
##Exercise 10
#a, pg 18
binom.test(35, 95, p = 0.33, alternative = "two.sided", conf.level = 0.95)
#two-sided as we don't know the direction above or below
#we cannot reject the null hypothesis
#so it looks like a third of all students will probs get an A
#b
binom.test(45, 110, p = 0.5, alternative = "two.sided", conf.level = 0.95)
#p-value = 0.07 so we cannot reject the null
# would maybe want a bigger sample size before we convincingly say half of
#students fail an exam
#c
binom.test(50, 300, p = 0.2, alternative = "two.sided", conf.level = 0.95)
#p-value = 0.17 so cannot reject the null
####Hypothesis test for a difference between two proportions
y1 <- 40
n1 <- 110
y2 <- 20
n2 <- 90
p1 <- y1/n1
p2 <- y2/n2
phat <- (y1 + y2)/ (n1 + n2)
z <- (p1-p2)/(sqrt(phat*(1-phat)*(1/n1+1/n2)))
z
alpha<-0.05 # the significance level
z.half.alpha<-qnorm(1-alpha/2)
c(-z.half.alpha,z.half.alpha) #standard normal is symmetric around 0
#the test statistic (2.17) does not lie between -1.96 and 1.96 so we can
#reject the null hypothesis
#to do this we need to test that np and n(1-p) are greater than or equal to
#5 for both for groups
#the alternative way
prop.test(x = c(40, 20), n = c(110, 90), correct = T)
#x values are the number with the certain out come i.e., are smokers
#n is the number in each sample
#correct = T says we should do a continuity correction
#The p-value is less than 0.05 so we can reject the null hypothesis
##Exercise 11
#a
prop.test(x = c(35, 40), n = c(95, 60), correct = T)
#p-value is 0.00055 which is less than 0.05 so we can rekect the null hypothesis
#so, we think they are not equal
#b
prop.test(x = c(5, 45), n = c(80, 110), correct = T, alternative = "less")
#alternative = less says we think the first one is lower than the second
#the p-value is 1.053e-07 which less than 0.05 so we can reject null
#and predict the first numbers are lower
#c
prop.test(x = c(100, 50), n = c(150, 300), correct = T, alternative = "greater")
#alternative = greater says we think the first one is lower than the second
#p-value is 2.2e-16 whihc less than 0.05 so we can reject the null
#and predict the first number is higher
#Hypothesis test for comparing counts in contingency tables
#to cehck if two variables (e.g., A and B) are independent from each other
#i.e., knowing level of variable A does not help predict B
#making the initial table
Blue_Eyes<- c(38, 14, 52) #create first column
Brown_Eyes<- c(11, 51, 62) #create second column
Row_Totals<- c(49, 65, 114) #third column
Table_1<- cbind(Blue_Eyes, Brown_Eyes, Row_Totals)#combine the columns
row.names(Table_1)<- c("Fair_Hair", "Dark_Hair", "Column_Totals") #give names to rows
Table_1
#under null hypothesis the two variables are assumed to be independent
#for example, we could calculate the probability if independent of having dark
#fair hair and brown eyes as 65/114 * 62/114 = 0.31
#Calculating the probabilities
Blue_Eyes<- c((49/114)*(52/114), (65/114)*(52/114), 52) #create first column
Brown_Eyes<- c((49/114)*(62/114), (65/114)*(62/114), 62) #create second column
Row_Totals<- c(49,65,114) #third column
Table_2=cbind(Blue_Eyes,Brown_Eyes,Row_Totals)#combine the columns
row.names(Table_2)<- c("Fair_Hair", "Dark_Hair", "Column_Totals")#give names to rows
Table_2
#EXPLANATION -yes, they are all calculated by doing multiplying the products
#as we assume they are independent
##Calculating the expected frequencies
Blue_Eyes<- c(114*(49/114)*(52/114), 114*(65/114)*(52/114), 52) #create first column
Brown_Eyes<- c(114*(49/114)*(62/114), 114*(65/114)*(62/114), 62) #create second column
Row_Totals<- c(49,65,114) #third column
Table_3<- cbind(Blue_Eyes, Brown_Eyes, Row_Totals)#combine the columns
row.names(Table_3)<- c("Fair_Hair", "Dark_Hair", "Column_Totals")#give names to rows
Table_3
#using the independent probabilties to calculate the expected frequencies
#Now take the observed frequencies and expected frequencies and calclulate using
#(O-E)^2/ E
#thus, to create the table
FairHair_And_BlueEyes<- c(38, 22.35, (38-22.35)^2/22.35)
FairHair_And_BrownEyes<- c(11, 26.65, (11-26.65)^2/26.65)
DarkHair_And_BlueEyes<- c(14, 29.65, (14-29.65)^2/29.65)
DarkHair_And_BrownEyes<- c(51, 35.35, (51-35.35)^2/35.35)
Table_4<- rbind(FairHair_And_BlueEyes, FairHair_And_BrownEyes, DarkHair_And_BlueEyes, DarkHair_And_BrownEyes)
colnames(Table_4)<- c("O", "E", "(O-E)^2/E")
Table_4
#sum all of them to get chi squared value
chi_sq = sum(10.96, 9.19, 8.26, 6.93)
chi_sq
#Now, we calculate the critical value
qchisq(0.95, 1) #or we can use qchisq(0.05, 1, lower.tail = F) from workshop 2
#The chi-sq value is larger than critical value, thus cannot accept null
#they probably are dependent
##easier way to do this in R]
count<- matrix(c(38,14,11,51), nrow=2) #create the matrix with the observed frequencies
count
chisq.test(count, correct=F) #chi-square test without Yates’s continuity correction
#p-value is very small 2.778e-09 which is lower than 0.05
#so we can reject the null that they are independent
##Exercise 12
#a
#count <- matrix(c())
count <- matrix(c(35, 98, 25, 90), nrow = 2)
count2 <- matrix(c(35, 98, 25, 90), nrow = 2)
count2
chisq.test(count2, correct = F)
count3 <- matrix(c(5, 25, 6, 59), nrow = 2)
count3
chisq.test(count3, correct = F)
chisq.test(count2, correct = F)
count3 <- matrix(c(5, 25, 6, 59), nrow = 2)
count3
chisq.test(count3, correct = F)
count4 <- matrix(c(50, 20, 40, 10), nrow = 2)
count4
count4 <- matrix(c(50, 20, 40, 10), nrow = 2)
count4
chisq.test(count4, correct = F)
Blue_Eyes<- c(6, 2, 8) #create first column
Brown_Eyes<- c(4, 8, 12) #create second column
Row_Totals<- c(10, 10, 20) #third column
Table_5<- cbind(Blue_Eyes, Brown_Eyes, Row_Totals)#combine the columns
row.names(Table_5)<- c("Fair_Hair", "Dark_Hair", "Column_Totals")#give names to rows
Table_5
x <- matrix(c(6, 2, 4, 8), ncol = 2)
x
fisher.test(x)
count5 <- matrix(c(5, 3, 8, 2), ncol = 2)
count5
count5 <- matrix(c(5, 3, 8, 2), nrow = 2)
count5
count5 <- matrix(c(5, 8, 3, 2), nrow = 2)
count5
fisher.test(count5)
count6 <- matrix(c(2, 1, 3, 2), nrow = 2)
count6
fisher.test(count6)
count7 <- matrix(c(5, 2, 4, 10), nrow = 2)
count7
fisher.test(count7)
library(ggplot2)
system.file(package='ggplot2')
install.packages("ggplot2")
R.version()
R.Version()
update.packages()
install.packages("ggplot2")
library(ggplot2)
install.packages("Rtools")
install.packages("installr")
library(installr)
sessionInfo()
install("ggplot2")
install("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
y <- c(13, 7, 5, 12, 9, 15, 6, 11, 9, 7, 12)
y - mean(y)
variance(y)
var(y)
View(data2)
library("ggplot2")
df <- read.csv('UCL_bar.csv')
View(yields)
packageVersion("dplyr")
packageVersion("stringr")
packageVersion("bacollite")
R.version()
R.Version()
file.path(R.home(), "bin", "R")
file.path(R.home(), "bin", "Rscript.exe")
which R
file.path(R.home())
file.path(R.home("bin"), "R")
file.path(R.home(), "bin", "Rscript.exe")
Rscript()
full_filepath <- file.path(R.home(), "bin", "Rscript.exe")
print(full_filepath)
# Variation.R
setwd('C:/Users/tobyl/OneDrive - The University of Manchester/Bioinformatics Masters/Research Project 2/development/RP2_gutFBA/first_model')
getwd()
abundances <- read.csv('Outputs/healthy_df_out.csv', header = T)
head(abundances)
# Variation.R
setwd('C:/Users/tobyl/OneDrive - The University of Manchester/Bioinformatics Masters/Research Project 2/development/RP2_gutFBA/first_model')
getwd()
abundances_df <- read.csv('Outputs/healthy_df_out.csv', header = T)
head(abundances_df)
View(abundances_df)
??rowMeans
rowMeans(abundances_df)
class(abundances_df)
??read.csv
View(abundances_df)
sapply(abundances_df, class)
rowMeans(abundances_df[:, 3:])
rowMeans(abundances_df[, 3:])
rowMeans(abundances_df[, 3:ncol(abundances_df)])
abundancesdf$mean <- rowMeans(abundances_df[, 3:ncol(abundances_df)])
abundances_df$mean <- rowMeans(abundances_df[, 3:ncol(abundances_df)])
View(abundances_df)
# Variation.R
setwd('C:/Users/tobyl/OneDrive - The University of Manchester/Bioinformatics Masters/Research Project 2/development/RP2_gutFBA/first_model')
getwd()
abundances_df <- read.csv('Outputs/healthy_df_out.csv', header = T)
abundances_df$mean <- rowMeans(abundances_df[, 3:ncol(abundances_df)])
head(abundances_df)
??apply(array, margin, ...)
??apply
abundances_df$mean <- rowMeans(abundances_df[, 3:ncol(abundances_df)])
abundances_df$SD <- apply(abundances_df, 1, sd, na.rm = TRUE)
head(abundances_df)
warnings()
View(abundances_df)
??rowwise
abundances_df <- read.csv('Outputs/healthy_df_out.csv', header = T)
abundances_df <- abundances_df %>%
rowwise() %>%
mutate(mean(c_across(where(is.numeric))))
library(dplyr)
setwd('C:/Users/tobyl/OneDrive - The University of Manchester/Bioinformatics Masters/Research Project 2/development/RP2_gutFBA/first_model')
getwd()
abundances_df <- read.csv('Outputs/healthy_df_out.csv', header = T)
abundances_df <- abundances_df %>%
rowwise() %>%
mutate(
mean(c_across(where(is.numeric))))
head(abundances_df)
abundances_df <- abundances_df %>%
rowwise() %>%
mutate(
mean = mean(c_across(where(is.numeric))))
#abundances_df$mean <- rowMeans(abundances_df[, 3:ncol(abundances_df)])
#abundances_df$SD <- apply(abundances_df, 1, sd, na.rm = TRUE)
head(abundances_df)
colnames(abundances_df)
abundances_df <- abundances_df %>%
rowwise() %>%
mutate(
mean = mean(c_across(where(is.numeric))),
sd = sd(c_across(where(is.numeric)))
)
#abundances_df$mean <- rowMeans(abundances_df[, 3:ncol(abundances_df)])
#abundances_df$SD <- apply(abundances_df, 1, sd, na.rm = TRUE)
colnames(abundances_df)
View(abundances_df)
